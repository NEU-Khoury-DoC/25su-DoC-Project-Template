{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55eb6d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest R²: 0.9233577682525691\n"
     ]
    }
   ],
   "source": [
    "# random forest for comparison, this tells us that maybe DNN is not the best model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "print(\"Random Forest R²:\", r2_score(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00660487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mupc\\anaconda3\\envs\\tf-env\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 1023.3794 - mae: 31.4452 - val_loss: 957.7689 - val_mae: 30.7362\n",
      "Epoch 2/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 885.7900 - mae: 29.3088 - val_loss: 900.7255 - val_mae: 29.7871\n",
      "Epoch 3/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 749.7896 - mae: 26.9808 - val_loss: 790.8032 - val_mae: 27.8689\n",
      "Epoch 4/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 629.8449 - mae: 24.5815 - val_loss: 605.0908 - val_mae: 24.1056\n",
      "Epoch 5/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 472.8452 - mae: 20.9160 - val_loss: 410.5809 - val_mae: 19.1492\n",
      "Epoch 6/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 321.7488 - mae: 16.5507 - val_loss: 238.1116 - val_mae: 13.2952\n",
      "Epoch 7/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 223.6053 - mae: 13.3771 - val_loss: 131.3680 - val_mae: 9.3461\n",
      "Epoch 8/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 152.0993 - mae: 10.5274 - val_loss: 77.6081 - val_mae: 7.1348\n",
      "Epoch 9/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 73.6004 - mae: 6.8329 - val_loss: 54.1848 - val_mae: 5.9376\n",
      "Epoch 10/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 75.3684 - mae: 6.8244 - val_loss: 35.2966 - val_mae: 5.0782\n",
      "Epoch 11/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 41.9377 - mae: 5.2918 - val_loss: 25.6694 - val_mae: 4.2768\n",
      "Epoch 12/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 46.0060 - mae: 5.5290 - val_loss: 22.6982 - val_mae: 3.8732\n",
      "Epoch 13/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 43.2249 - mae: 5.1538 - val_loss: 15.8224 - val_mae: 3.2363\n",
      "Epoch 14/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 35.7623 - mae: 4.5757 - val_loss: 17.2071 - val_mae: 3.4199\n",
      "Epoch 15/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 28.5640 - mae: 4.1315 - val_loss: 14.6976 - val_mae: 3.0607\n",
      "Epoch 16/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 27.8291 - mae: 3.8961 - val_loss: 16.2488 - val_mae: 3.2965\n",
      "Epoch 17/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 48.8332 - mae: 5.5364 - val_loss: 10.8892 - val_mae: 2.7332\n",
      "Epoch 18/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 32.7981 - mae: 4.5451 - val_loss: 11.5672 - val_mae: 2.6768\n",
      "Epoch 19/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 25.2158 - mae: 3.9400 - val_loss: 13.3424 - val_mae: 2.8741\n",
      "Epoch 20/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 31.7217 - mae: 4.4392 - val_loss: 11.5974 - val_mae: 2.7384\n",
      "Epoch 21/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 18.3475 - mae: 3.3199 - val_loss: 9.6686 - val_mae: 2.5587\n",
      "Epoch 22/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 19.6201 - mae: 3.5449 - val_loss: 12.1804 - val_mae: 2.8278\n",
      "Epoch 23/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 35.6766 - mae: 4.7226 - val_loss: 10.8437 - val_mae: 2.6063\n",
      "Epoch 24/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 27.3306 - mae: 3.8455 - val_loss: 9.6849 - val_mae: 2.4788\n",
      "Epoch 25/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 22.2277 - mae: 3.4852 - val_loss: 9.5264 - val_mae: 2.4686\n",
      "Epoch 26/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 22.9586 - mae: 3.6444 - val_loss: 10.2394 - val_mae: 2.5192\n",
      "Epoch 27/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 20.9104 - mae: 3.5215 - val_loss: 9.4472 - val_mae: 2.4825\n",
      "Epoch 28/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 21.7234 - mae: 3.8173 - val_loss: 8.7355 - val_mae: 2.4771\n",
      "Epoch 29/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 16.5357 - mae: 3.1705 - val_loss: 8.4309 - val_mae: 2.4363\n",
      "Epoch 30/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 17.8040 - mae: 3.1141 - val_loss: 8.6661 - val_mae: 2.3551\n",
      "Epoch 31/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 20.9742 - mae: 3.5436 - val_loss: 9.8653 - val_mae: 2.4042\n",
      "Epoch 32/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 18.3366 - mae: 3.2978 - val_loss: 10.4443 - val_mae: 2.5386\n",
      "Epoch 33/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 15.4156 - mae: 3.0626 - val_loss: 8.9267 - val_mae: 2.3525\n",
      "Epoch 34/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 15.8923 - mae: 3.2231 - val_loss: 8.4211 - val_mae: 2.3132\n",
      "Epoch 35/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 20.9881 - mae: 3.5793 - val_loss: 9.1996 - val_mae: 2.3194\n",
      "Epoch 36/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 15.7006 - mae: 2.9687 - val_loss: 7.5858 - val_mae: 2.0950\n",
      "Epoch 37/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 15.5083 - mae: 3.0162 - val_loss: 9.5599 - val_mae: 2.2799\n",
      "Epoch 38/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 23.8449 - mae: 3.6862 - val_loss: 9.1277 - val_mae: 2.3094\n",
      "Epoch 39/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13.2704 - mae: 2.8630 - val_loss: 9.5539 - val_mae: 2.3265\n",
      "Epoch 40/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 16.3827 - mae: 3.1407 - val_loss: 8.8051 - val_mae: 2.1826\n",
      "Epoch 41/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 16.4149 - mae: 3.3018 - val_loss: 8.9340 - val_mae: 2.2982\n",
      "Epoch 42/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 14.6013 - mae: 3.0510 - val_loss: 7.9334 - val_mae: 2.1653\n",
      "Epoch 43/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 14.5285 - mae: 3.0318 - val_loss: 9.0687 - val_mae: 2.3347\n",
      "Epoch 44/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 12.3035 - mae: 2.6214 - val_loss: 8.7971 - val_mae: 2.3500\n",
      "Epoch 45/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 16.3534 - mae: 3.2883 - val_loss: 7.5771 - val_mae: 2.0328\n",
      "Epoch 46/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12.2190 - mae: 2.8145 - val_loss: 8.1256 - val_mae: 2.1218\n",
      "Epoch 47/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 15.5964 - mae: 3.1059 - val_loss: 8.0636 - val_mae: 2.1805\n",
      "Epoch 48/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.7258 - mae: 2.5731 - val_loss: 8.6326 - val_mae: 2.2436\n",
      "Epoch 49/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.8703 - mae: 2.6159 - val_loss: 8.6925 - val_mae: 2.2601\n",
      "Epoch 50/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 12.1187 - mae: 2.8489 - val_loss: 9.8458 - val_mae: 2.3433\n",
      "Epoch 51/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11.1861 - mae: 2.5514 - val_loss: 9.4229 - val_mae: 2.3217\n",
      "Epoch 52/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.7070 - mae: 2.6472 - val_loss: 7.8093 - val_mae: 2.2548\n",
      "Epoch 53/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13.8394 - mae: 2.9002 - val_loss: 7.2768 - val_mae: 2.1879\n",
      "Epoch 54/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.2370 - mae: 2.4017 - val_loss: 7.2843 - val_mae: 2.2287\n",
      "Epoch 55/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.3577 - mae: 2.6413 - val_loss: 7.6561 - val_mae: 2.1750\n",
      "Epoch 56/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.5718 - mae: 2.5486 - val_loss: 8.1245 - val_mae: 2.2383\n",
      "Epoch 57/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13.2782 - mae: 2.8517 - val_loss: 7.7114 - val_mae: 2.1879\n",
      "Epoch 58/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.0692 - mae: 2.5868 - val_loss: 7.2342 - val_mae: 2.1022\n",
      "Epoch 59/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.5479 - mae: 2.2172 - val_loss: 7.0952 - val_mae: 2.0150\n",
      "Epoch 60/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.8493 - mae: 2.1732 - val_loss: 6.7388 - val_mae: 1.9134\n",
      "Epoch 61/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.7426 - mae: 2.5798 - val_loss: 6.2189 - val_mae: 1.8661\n",
      "Epoch 62/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.4561 - mae: 2.1785 - val_loss: 6.7648 - val_mae: 1.8891\n",
      "Epoch 63/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.0058 - mae: 2.3323 - val_loss: 6.8969 - val_mae: 1.8860\n",
      "Epoch 64/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.3931 - mae: 2.5925 - val_loss: 7.3292 - val_mae: 2.0670\n",
      "Epoch 65/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.4242 - mae: 2.4365 - val_loss: 7.8771 - val_mae: 1.9525\n",
      "Epoch 66/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.0135 - mae: 2.1610 - val_loss: 10.4187 - val_mae: 2.2584\n",
      "Epoch 67/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.1154 - mae: 2.4007 - val_loss: 9.8152 - val_mae: 2.4703\n",
      "Epoch 68/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.9094 - mae: 2.2114 - val_loss: 9.2784 - val_mae: 2.3204\n",
      "Epoch 69/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.0400 - mae: 2.4310 - val_loss: 8.7492 - val_mae: 2.2595\n",
      "Epoch 70/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11.1761 - mae: 2.4938 - val_loss: 7.9327 - val_mae: 2.0330\n",
      "Epoch 71/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11.1199 - mae: 2.5899 - val_loss: 8.3977 - val_mae: 2.1072\n",
      "Epoch 72/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.0254 - mae: 2.4126 - val_loss: 9.1022 - val_mae: 2.1479\n",
      "Epoch 73/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.5500 - mae: 2.6369 - val_loss: 8.4847 - val_mae: 2.1527\n",
      "Epoch 74/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.1301 - mae: 2.4128 - val_loss: 8.3486 - val_mae: 2.1619\n",
      "Epoch 75/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.5395 - mae: 2.0204 - val_loss: 8.3341 - val_mae: 2.1029\n",
      "Epoch 76/300\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.1728 - mae: 2.0894 - val_loss: 9.0356 - val_mae: 2.2233\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 10.1199 - mae: 2.1933\n",
      "Test MAE: 2.23\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "Predicted Gini: 28.19\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "df = pd.read_csv('../datasets/MEGAFRAME_CLEANEDV2.csv')\n",
    "\n",
    "# Remove both targets and irrelevant columns from features\n",
    "X = df.drop(columns=['Gini index', 'UNEMP', 'Reference area', 'REF_AREA', 'Region'])\n",
    "y = df['Gini index']\n",
    "\n",
    "categorical_features = ['TIME_PERIOD']\n",
    "numerical_features = X.columns.difference(categorical_features)\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', StandardScaler(), numerical_features),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "])\n",
    "\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n",
    "# Chat GPT gave me the idea to do droupout layers, the lower I used the better the model performed\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(X_train.shape[1],)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.1),\n",
    "    Dense(64, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.1),\n",
    "    Dense(32, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "    BatchNormalization(),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=300, batch_size=8, validation_split=0.1, callbacks=[early_stop])\n",
    "\n",
    "loss, mae = model.evaluate(X_test, y_test)\n",
    "print(f\"Test MAE: {mae:.2f}\")\n",
    "\n",
    "new_data = pd.DataFrame({\n",
    "    'TIME_PERIOD': [2001],\n",
    "    'Trade union density': [78.699997],\n",
    "    'Combined corporate income tax rate': [28.0],\n",
    "    'Education spending': [0.0734319847255705],\n",
    "    'Health spending': [0.0631525528524754],\n",
    "    'Housing spending': [0.0057497428086187],\n",
    "    'Community development spending': [0.0025634702523358],\n",
    "    'IRLT': [5.1075],\n",
    "    'Population, total': [8895960.0],\n",
    "    'GDP per capita (current US$)': [27259.4806735435],\n",
    "    'Inflation, consumer prices (annual %)': [2.40595834145438]\n",
    "})\n",
    "\n",
    "new_data_processed = preprocessor.transform(new_data)\n",
    "predicted_gini = model.predict(new_data_processed)\n",
    "print(f\"Predicted Gini: {predicted_gini.flatten()[0]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "029a6bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "MAE: 2.23\n",
      "MSE: 10.81\n",
      "RMSE: 3.29\n",
      "R² Score: 0.69\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"MSE: {mse:.2f}\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"R² Score: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4ccd6f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mupc\\anaconda3\\envs\\tf-env\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mupc\\anaconda3\\envs\\tf-env\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mupc\\anaconda3\\envs\\tf-env\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mupc\\anaconda3\\envs\\tf-env\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mupc\\anaconda3\\envs\\tf-env\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Mean MAE over 5 folds: 1.93 ± 0.16\n",
      "Mean R² over 5 folds: 0.7293 ± 0.0907\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "mae_scores = []\n",
    "r2_scores = []\n",
    "\n",
    "for train_index, val_index in kf.split(X_processed):\n",
    "    X_train, X_val = X_processed[train_index], X_processed[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(X_train.shape[1],)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.1),\n",
    "        Dense(64, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.1),\n",
    "        Dense(32, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "        BatchNormalization(),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    \n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "    \n",
    "    model.fit(X_train, y_train, epochs=300, batch_size=8, validation_data=(X_val, y_val), callbacks=[early_stop], verbose=0)\n",
    "    \n",
    "    y_val_pred = model.predict(X_val).flatten()\n",
    "    \n",
    "    mae = mean_absolute_error(y_val, y_val_pred)\n",
    "    r2 = r2_score(y_val, y_val_pred)\n",
    "    \n",
    "    mae_scores.append(mae)\n",
    "    r2_scores.append(r2)\n",
    "\n",
    "print(f\"Mean MAE over 5 folds: {np.mean(mae_scores):.2f} ± {np.std(mae_scores):.2f}\")\n",
    "print(f\"Mean R² over 5 folds: {np.mean(r2_scores):.4f} ± {np.std(r2_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "34fe4274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Statistics:\n",
      "       TIME_PERIOD  Trade union density  Combined corporate income tax rate  \\\n",
      "count   400.000000           400.000000                          400.000000   \n",
      "mean   2010.605000            32.947000                           25.384909   \n",
      "std       5.088094            21.854253                            6.892238   \n",
      "min    2000.000000             7.100000                            9.000000   \n",
      "25%    2006.750000            16.600000                           20.000000   \n",
      "50%    2011.000000            25.050000                           25.000000   \n",
      "75%    2015.000000            49.825000                           30.000000   \n",
      "max    2020.000000            91.599998                           51.611553   \n",
      "\n",
      "       Education spending  Health spending  Housing spending  \\\n",
      "count          400.000000       400.000000        400.000000   \n",
      "mean             0.052006         0.064127          0.003168   \n",
      "std              0.014906         0.017721          0.003309   \n",
      "min              0.000265         0.000265          0.000000   \n",
      "25%              0.042913         0.057009          0.000588   \n",
      "50%              0.051790         0.068806          0.002062   \n",
      "75%              0.061203         0.074686          0.004654   \n",
      "max              0.096981         0.101046          0.013970   \n",
      "\n",
      "       Community development spending        IRLT       UNEMP  \\\n",
      "count                      400.000000  400.000000  400.000000   \n",
      "mean                         0.001938    3.476154    7.925227   \n",
      "std                          0.001688    2.254951    3.963356   \n",
      "min                          0.000000   -0.362000    2.266667   \n",
      "25%                          0.000575    1.711250    5.141667   \n",
      "50%                          0.001723    3.675333    7.125000   \n",
      "75%                          0.002610    4.534250    9.205833   \n",
      "max                          0.007645   14.004417   27.825000   \n",
      "\n",
      "       Population, total  GDP per capita (current US$)  \\\n",
      "count       4.000000e+02                    400.000000   \n",
      "mean        2.170435e+07                  40980.009757   \n",
      "std         2.609184e+07                  24369.878616   \n",
      "min         2.895210e+05                   5250.972924   \n",
      "25%         4.883768e+06                  20859.216937   \n",
      "50%         9.183864e+06                  40591.731459   \n",
      "75%         3.815506e+07                  51629.310291   \n",
      "max         1.280700e+08                 123678.702143   \n",
      "\n",
      "       Inflation, consumer prices (annual %)  Gini index  \n",
      "count                             400.000000  400.000000  \n",
      "mean                                2.069098   31.465000  \n",
      "std                                 1.911858    5.215773  \n",
      "min                                -4.447547   23.200000  \n",
      "25%                                 0.928950   27.675000  \n",
      "50%                                 1.920316   30.800000  \n",
      "75%                                 2.761289   34.125000  \n",
      "max                                15.402319   54.600000  \n",
      "\n",
      "Missing Values:\n",
      "TIME_PERIOD                              0\n",
      "Reference area                           0\n",
      "REF_AREA                                 0\n",
      "Trade union density                      0\n",
      "Combined corporate income tax rate       0\n",
      "Education spending                       0\n",
      "Health spending                          0\n",
      "Housing spending                         0\n",
      "Community development spending           0\n",
      "IRLT                                     0\n",
      "UNEMP                                    0\n",
      "Population, total                        0\n",
      "GDP per capita (current US$)             0\n",
      "Inflation, consumer prices (annual %)    0\n",
      "Gini index                               0\n",
      "Region                                   0\n",
      "dtype: int64\n",
      "\n",
      "Data Types:\n",
      "TIME_PERIOD                                int64\n",
      "Reference area                            object\n",
      "REF_AREA                                  object\n",
      "Trade union density                      float64\n",
      "Combined corporate income tax rate       float64\n",
      "Education spending                       float64\n",
      "Health spending                          float64\n",
      "Housing spending                         float64\n",
      "Community development spending           float64\n",
      "IRLT                                     float64\n",
      "UNEMP                                    float64\n",
      "Population, total                        float64\n",
      "GDP per capita (current US$)             float64\n",
      "Inflation, consumer prices (annual %)    float64\n",
      "Gini index                               float64\n",
      "Region                                    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Summary Statistics:\")\n",
    "print(df.describe())\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\nData Types:\")\n",
    "print(df.dtypes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
